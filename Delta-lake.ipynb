{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/pee.tankulrat/Develop/TW/talk/delta-lake/.venv/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/pee.tankulrat/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/pee.tankulrat/.ivy2/jars\n",
      "org.apache.hudi#hudi-spark3.4-bundle_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c671e21c-9de4-430b-9570-c6db644b4b8a;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hudi#hudi-spark3.4-bundle_2.12;0.14.1 in central\n",
      ":: resolution report :: resolve 72ms :: artifacts dl 1ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.hudi#hudi-spark3.4-bundle_2.12;0.14.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c671e21c-9de4-430b-9570-c6db644b4b8a\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/5ms)\n",
      "24/03/17 16:36:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/17 16:36:03 WARN SparkSession: Cannot use org.apache.spark.sql.hudi.HoodieSparkSessionExtension to configure session extensions.\n",
      "java.lang.ClassNotFoundException: org.apache.spark.sql.adapter.Spark2Adapter\n",
      "\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:593)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)\n",
      "\tat org.apache.hudi.SparkAdapterSupport$.sparkAdapter$lzycompute(SparkAdapterSupport.scala:49)\n",
      "\tat org.apache.hudi.SparkAdapterSupport$.sparkAdapter(SparkAdapterSupport.scala:35)\n",
      "\tat org.apache.hudi.SparkAdapterSupport.sparkAdapter(SparkAdapterSupport.scala:29)\n",
      "\tat org.apache.hudi.SparkAdapterSupport.sparkAdapter$(SparkAdapterSupport.scala:29)\n",
      "\tat org.apache.spark.sql.hudi.HoodieSparkSessionExtension.sparkAdapter$lzycompute(HoodieSparkSessionExtension.scala:28)\n",
      "\tat org.apache.spark.sql.hudi.HoodieSparkSessionExtension.sparkAdapter(HoodieSparkSessionExtension.scala:28)\n",
      "\tat org.apache.spark.sql.hudi.HoodieSparkSessionExtension.apply(HoodieSparkSessionExtension.scala:54)\n",
      "\tat org.apache.spark.sql.hudi.HoodieSparkSessionExtension.apply(HoodieSparkSessionExtension.scala:28)\n",
      "\tat org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1370)\n",
      "\tat org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)\n",
      "\tat org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/17 16:36:15 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Hudi\")\n",
    "    .config(\n",
    "        \"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0\"\n",
    "    )\n",
    "    .config(\n",
    "        \"spark.sql.extensions\",\n",
    "        \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "    )\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.iceberg.spark.SparkSessionCatalog\",\n",
    "    )\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog.type\",\n",
    "        \"hive\",\n",
    "    )\n",
    "    .config(\n",
    "        \"spark.sql.catalog.local\",\n",
    "        \"org.apache.iceberg.spark.SparkCatalog\",\n",
    "    )\n",
    "    .config(\n",
    "        \"spark.sql.catalog.local.type\",\n",
    "        \"hive\",\n",
    "    )\n",
    "    .config(\n",
    "        \"spark.sql.catalog.local.uri\",\n",
    "        \"thrift://localhost:9083\",\n",
    "    )\n",
    "    \n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "    .config(\"spark.driver.memory\", \"2g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m----> 4\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msc://localhost:15002\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Develop/TW/talk/delta-lake/.venv/lib/python3.12/site-packages/pyspark/sql/session.py:479\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    477\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSPARK_CONNECT_MODE_ENABLED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    478\u001b[0m     opts[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.remote\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m url\n\u001b[0;32m--> 479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRemoteSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSPARK_LOCAL_REMOTE\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    481\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msc://localhost\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Develop/TW/talk/delta-lake/.venv/lib/python3.12/site-packages/pyspark/sql/connect/session.py:220\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    218\u001b[0m     session \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39m_default_session\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m session \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 220\u001b[0m         session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_options(session)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m session\n",
      "File \u001b[0;32m~/Develop/TW/talk/delta-lake/.venv/lib/python3.12/site-packages/pyspark/sql/connect/session.py:211\u001b[0m, in \u001b[0;36mSparkSession.Builder.create\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    208\u001b[0m     session \u001b[38;5;241m=\u001b[39m SparkSession(connection\u001b[38;5;241m=\u001b[39mspark_remote)\n\u001b[1;32m    210\u001b[0m SparkSession\u001b[38;5;241m.\u001b[39m_set_default_and_active_session(session)\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m session\n",
      "File \u001b[0;32m~/Develop/TW/talk/delta-lake/.venv/lib/python3.12/site-packages/pyspark/sql/connect/session.py:183\u001b[0m, in \u001b[0;36mSparkSession.Builder._apply_options\u001b[0;34m(self, session)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 183\u001b[0m         \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    185\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[0;32m~/Develop/TW/talk/delta-lake/.venv/lib/python3.12/site-packages/pyspark/sql/connect/conf.py:41\u001b[0m, in \u001b[0;36mRuntimeConf.set\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m     39\u001b[0m op_set \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mConfigRequest\u001b[38;5;241m.\u001b[39mSet(pairs\u001b[38;5;241m=\u001b[39m[proto\u001b[38;5;241m.\u001b[39mKeyValue(key\u001b[38;5;241m=\u001b[39mkey, value\u001b[38;5;241m=\u001b[39mvalue)])\n\u001b[1;32m     40\u001b[0m operation \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mConfigRequest\u001b[38;5;241m.\u001b[39mOperation(\u001b[38;5;28mset\u001b[39m\u001b[38;5;241m=\u001b[39mop_set)\n\u001b[0;32m---> 41\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m warn \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39mwarnings:\n\u001b[1;32m     43\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(warn)\n",
      "File \u001b[0;32m~/Develop/TW/talk/delta-lake/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1368\u001b[0m, in \u001b[0;36mSparkConnectClient.config\u001b[0;34m(self, operation)\u001b[0m\n\u001b[1;32m   1366\u001b[0m req\u001b[38;5;241m.\u001b[39moperation\u001b[38;5;241m.\u001b[39mCopyFrom(operation)\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1368\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mattempt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retrying\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mattempt\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_builder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Develop/TW/talk/delta-lake/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1684\u001b[0m, in \u001b[0;36mRetrying.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1681\u001b[0m             backoff \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jitter)\n\u001b[1;32m   1683\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying call after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackoff\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ms sleep\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1684\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackoff\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1685\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m AttemptManager(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_can_retry, retry_state)\n\u001b[1;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m retry_state\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m   1688\u001b[0m     \u001b[38;5;66;03m# Exceeded number of retries, throw last exception we had\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.remote(\"sc://localhost:15002\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/pee.tankulrat/Develop/TW/talk/delta-lake/.venv/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/pee.tankulrat/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/pee.tankulrat/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-07b83352-f651-4f73-9549-5e528a1fcc5e;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.1.0 in central\n",
      "\tfound io.delta#delta-storage;3.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 103ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.1.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-07b83352-f651-4f73-9549-5e528a1fcc5e\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/2ms)\n",
      "24/03/19 10:08:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/19 10:08:57 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "builder = (\n",
    "    SparkSession.builder.appName(\"MyApp\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    .config(\"spark.executor.memory\", \"12g\")\n",
    "    .config(\"spark.driver.memory\", \"12g\")\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/17 16:02:42 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/03/17 16:02:42 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "24/03/17 16:02:42 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "24/03/17 16:02:42 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "24/03/17 16:02:42 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore pee.tankulrat@192.168.1.52\n",
      "24/03/17 16:02:42 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/17 16:02:43 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/17 18:00:02 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "24/03/17 18:00:02 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "actors = spark.read.option(\"header\", \"true\").csv(\"data/letterboxd/actors.csv\")\n",
    "actors.write.format(\"delta\").mode(\"overwrite\").save(\"./data/delta/letterboxd/actors\")\n",
    "\n",
    "countries = spark.read.option(\"header\", \"true\").csv(\"data/letterboxd/countries.csv\")\n",
    "countries.write.format(\"delta\").mode(\"overwrite\").save(\n",
    "    \"./data/delta/letterboxd/countries\"\n",
    ")\n",
    "\n",
    "crew = spark.read.option(\"header\", \"true\").csv(\"data/letterboxd/crew.csv\")\n",
    "crew.write.format(\"delta\").mode(\"overwrite\").save(\"./data/delta/letterboxd/crew\")\n",
    "\n",
    "genres = spark.read.option(\"header\", \"true\").csv(\"data/letterboxd/genres.csv\")\n",
    "genres.write.format(\"delta\").mode(\"overwrite\").save(\"./data/delta/letterboxd/genres\")\n",
    "\n",
    "languages = spark.read.option(\"header\", \"true\").csv(\"data/letterboxd/languages.csv\")\n",
    "languages.write.format(\"delta\").mode(\"overwrite\").save(\n",
    "    \"./data/delta/letterboxd/languages\"\n",
    ")\n",
    "\n",
    "movies = spark.read.option(\"header\", \"true\").csv(\"data/letterboxd/movies.csv\")\n",
    "movies.write.format(\"delta\").mode(\"overwrite\").save(\"./data/delta/letterboxd/movies\")\n",
    "\n",
    "releases = spark.read.option(\"header\", \"true\").csv(\"data/letterboxd/releases.csv\")\n",
    "releases.write.format(\"delta\").mode(\"overwrite\").save(\n",
    "    \"./data/delta/letterboxd/releases\"\n",
    ")\n",
    "\n",
    "studios = spark.read.option(\"header\", \"true\").csv(\"data/letterboxd/studios.csv\")\n",
    "studios.write.format(\"delta\").mode(\"overwrite\").save(\"./data/delta/letterboxd/studios\")\n",
    "\n",
    "themes = spark.read.option(\"header\", \"true\").csv(\"data/letterboxd/themes.csv\")\n",
    "themes.write.format(\"delta\").mode(\"overwrite\").save(\"./data/delta/letterboxd/themes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/19 09:07:02 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "hotels = spark.read.option(\"header\", \"true\").csv(\"data/hotels.csv\")\n",
    "hotels = hotels.withColumnRenamed(\" countyName\", \"countyName\")\n",
    "hotels = hotels.withColumnRenamed(\" cityCode\", \"cityCode\")\n",
    "hotels = hotels.withColumnRenamed(\" cityName\", \"cityName\")\n",
    "hotels = hotels.withColumnRenamed(\" HotelCode\", \"HotelCode\")\n",
    "hotels = hotels.withColumnRenamed(\" HotelName\", \"HotelName\")\n",
    "hotels = hotels.withColumnRenamed(\" HotelRating\", \"HotelRating\")\n",
    "hotels = hotels.withColumnRenamed(\" Address\", \"Address\")\n",
    "hotels = hotels.withColumnRenamed(\" Attractions\", \"Attractions\")\n",
    "hotels = hotels.withColumnRenamed(\" Description\", \"Description\")\n",
    "hotels = hotels.withColumnRenamed(\" FaxNumber\", \"FaxNumber\")\n",
    "hotels = hotels.withColumnRenamed(\" HotelFacilities\", \"HotelFacilities\")\n",
    "hotels = hotels.withColumnRenamed(\" Map\", \"Map\")\n",
    "hotels = hotels.withColumnRenamed(\" PinCode\", \"PinCode\")\n",
    "hotels = hotels.withColumnRenamed(\" PhoneNumber\", \"PhoneNumber\")\n",
    "hotels = hotels.withColumnRenamed(\" HotelWebsiteUrl\", \"HotelWebsiteUrl\")\n",
    "# hotels.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- countyCode: string (nullable = true)\n",
      " |-- countyName: string (nullable = true)\n",
      " |-- cityCode: string (nullable = true)\n",
      " |-- cityName: string (nullable = true)\n",
      " |-- HotelCode: string (nullable = true)\n",
      " |-- HotelName: string (nullable = true)\n",
      " |-- HotelRating: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Attractions: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- FaxNumber: string (nullable = true)\n",
      " |-- HotelFacilities: string (nullable = true)\n",
      " |-- Map: string (nullable = true)\n",
      " |-- PhoneNumber: string (nullable = true)\n",
      " |-- PinCode: string (nullable = true)\n",
      " |-- HotelWebsiteUrl: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hotels.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# hotels.write.format(\"delta\").mode(\"overwrite\").save(\"./data/delta/hotels\")\n",
    "# hotels.saveAsTable(\"raw_hotels\")\n",
    "# hotels.write.format(\"delta\").saveAsTable(\"raw_hotels\")\n",
    "# hotels.writeTo(\"raw.hotels\").append()\n",
    "hotels.write.parquet(\"data/hotels.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+\n",
      "|namespace|tableName |isTemporary|\n",
      "+---------+----------+-----------+\n",
      "|default  |raw_hotels|false      |\n",
      "+---------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3316231"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotels.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1425577"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotelsWithCode = hotels.filter(hotels.HotelCode.isNotNull()).filter(\n",
    "    hotels.cityName.isNotNull()\n",
    ")\n",
    "hotelsWithCode.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---------------+-----------+--------------------+-----------+--------------------+---------+---------------+----+-----------+-------+---------------+\n",
      "|          countyCode|          countyName|            cityCode|            cityName|           HotelCode|      HotelName|HotelRating|             Address|Attractions|         Description|FaxNumber|HotelFacilities| Map|PhoneNumber|PinCode|HotelWebsiteUrl|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---------------+-----------+--------------------+-----------+--------------------+---------+---------------+----+-----------+-------+---------------+\n",
      "|                  AL|             Albania|              106078|            Albanien|             1003300| De Paris Hotel|   FourStar|Nr. 7 Brigada Vii...|       NULL|Hotel de Paris is...|     NULL|           NULL|NULL|       NULL|   NULL|           NULL|\n",
      "|Rooms and suites ...| a minibar and a ...| a bathrobe and f...|                NULL|                NULL|           NULL|       NULL|                NULL|       NULL|                NULL|     NULL|           NULL|NULL|       NULL|   NULL|           NULL|\n",
      "|The hotel is situ...| and just a few s...|  Palace of Congress|         Opera House| Art Gallery and ...|           NULL|       NULL|                NULL|       NULL|                NULL|     NULL|           NULL|NULL|       NULL|   NULL|           NULL|\n",
      "|             Ironing| laundry and dry ...|                NULL|                NULL|                NULL|           NULL|       NULL|                NULL|       NULL|                NULL|     NULL|           NULL|NULL|       NULL|   NULL|           NULL|\n",
      "|The nearest bus s...| while the Main B...|            42268822|Private parking P...|   41.32213|19.81665|00355 4226 5009|       1000|https://www.booki...|       NULL|                NULL|     NULL|           NULL|NULL|       NULL|   NULL|           NULL|\n",
      "|                  AL|             Albania|              106078|            Albanien|             1003301|    Hotel Green|   FourStar|Rruga Kavajes. Ko...|       NULL|Located in a subu...|     NULL|           NULL|NULL|       NULL|   NULL|           NULL|\n",
      "|All rooms are air...| satellite TV and...|                NULL|                NULL|                NULL|           NULL|       NULL|                NULL|       NULL|                NULL|     NULL|           NULL|NULL|       NULL|   NULL|           NULL|\n",
      "|Breakfast is serv...|                NULL|                NULL|                NULL|                NULL|           NULL|       NULL|                NULL|       NULL|                NULL|     NULL|           NULL|NULL|       NULL|   NULL|           NULL|\n",
      "|The centre of Tir...| an upmarket part...| can be reached i...|               shops|         restaurants|    trendy bars|       pubs| and cafes can be...|       NULL|                NULL|     NULL|           NULL|NULL|       NULL|   NULL|           NULL|\n",
      "|Main Bus Station ...| while the Train ...|         35548520058|airport pick up w...|   41.30413|19.74703|   +35548520057|       1041|https://www.booki...|       NULL|                NULL|     NULL|           NULL|NULL|       NULL|   NULL|           NULL|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---------------+-----------+--------------------+-----------+--------------------+---------+---------------+----+-----------+-------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hotels.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3316231"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotels.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/17 13:37:23 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3316231"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotelsDf = spark.read.format(\"delta\").load(\"./data/delta/hotels\")\n",
    "hotelsDf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('countyCode', StringType(), True), StructField('countyName', StringType(), True), StructField('cityCode', StringType(), True), StructField('cityName', StringType(), True), StructField('HotelCode', StringType(), True), StructField('HotelName', StringType(), True), StructField('HotelRating', StringType(), True), StructField('Address', StringType(), True), StructField('Attractions', StringType(), True), StructField('Description', StringType(), True), StructField('FaxNumber', StringType(), True), StructField('HotelFacilities', StringType(), True), StructField('Map', StringType(), True), StructField('PhoneNumber', StringType(), True), StructField('PinCode', StringType(), True), StructField('HotelWebsiteUrl', StringType(), True)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotelsDf.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eCommerce = spark.read.option(\"header\", \"true\").csv(\"data/eCommerce/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# TODO: Make this idempotent\n",
    "eCommerce.write.format(\"delta\").mode('overwrite').save(\"./data/delta/eCommerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109950743"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eComDelta = spark.read.format(\"delta\").load(\"./data/delta/eCommerce\")\n",
    "eComDelta.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterNullCategory(df):\n",
    "    return df.filter(df)\n",
    "\n",
    "def filterNullBrand(df):\n",
    "    return df.filter(df)\n",
    "\n",
    "def findTopSpender(df):\n",
    "    return df.filter(df)\n",
    "\n",
    "ecomCleaned = filterNullCategory(eComDelta)\n",
    "ecomCleaned = filterNullBrand(ecomCleaned)\n",
    "topSpenders = findTopSpender(ecomCleaned)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
